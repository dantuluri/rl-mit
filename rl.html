<!DOCTYPE html>
<script>window.texme = { style: 'none' }</script>
<script src="https://cdn.jsdelivr.net/npm/texme@0.5.0"></script>
<style>
body {
  background: lightcyan;
}
main {
  max-width: 50em;
  padding: 1em;
  border: medium double gray;
  margin: 2em auto;
  background: lightyellow;
}
</style>
<textarea>

# Reinforcement Learning (Supplemental Material)
## Surya Dantuluri October 2018

In reinforcement learning, there is an agent an environment.

    
![](one2.png)


Here, the robot is the agent running on a policy $$\pi(s)$$ in order to **maximize the reward it gets**
    
The robot takes actions where $$a\in A$$ which changes the states in which $$s\in S$$ in the environment.
No matter how good or bad the action is, the robot recieves a reward function, denoted as $$r\in R$$.
    
The way this works is a somewhat baseline of how RL generally works. Of course, reward functions are far more complicated most of the time
as well as what actions and what states the robot in this instance can take.
    
    
Different categories of RL Algorithms:
    
* Model-based: Rely on the model of the environment; either the model is known or the algorithm learns it explicitly.
* Model-free: No dependency on the model during learning.
* On-policy: Use the deterministic outcomes or samples from the target policy to train the algorithm.
* Off-policy: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy.
    
(Source: Lilian Weng: A Long Peek into RL)
    
Every episode is essentially a combination of a new:
    
* State $$s$$
* Action $$a$$
* Next State $$s'$$
* Reward Depending on value function $$r$$
    
By doing so, we are optimizing the policy $$\pi(s)$$
    
## Markov Decision Processes


![](clear.png)
(Sec. 3.1 Sutton & Barto (2017))
    
Essentially in a MDP all states have a Markov property. This means that none of the states rely on their past actions
or history, rather only rely on current state.
    

## Proximal Policy Optimization
#### Authored by John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov
    
"PPO strikes a balance between ease of implementation, sample complexity, and ease of tuning, trying to compute an update at each step that minimizes the cost function while ensuring the deviation from the previous policy is relatively small."
    
![](ppo.png)
    
![](pporobo.png)

(Source: https://blog.openai.com/openai-baselines-ppo/)
    
